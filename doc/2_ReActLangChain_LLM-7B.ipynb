{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f94d8c",
   "metadata": {},
   "source": [
    "# Prompt Engineering LLM with Learning by Examples\n",
    "\n",
    "Adapter le modele génératif de langage pour ajouter un module \"auto-critic\". Concretement, cela consiste à ajouter une sortie à la derniere sequence de decodeur. Entrainer le modele en figeant les poids du LLM (TransfertLearning), puis fineTuner le modele complet.\n",
    "\n",
    "![LLM-Critic](LLM-AutoCritic.png)\n",
    "\n",
    "**Dans cette deuxième étape, nous explorerons l'interaction entre le modèle de langage pré-entraîné (LLM) et une source externe à travers un exemple d'application. Nous aborderons également la manière de structurer les \"prompts\" de manière optimale pour atteindre notre objectif. Il est important de noter que ce document doit être exécuté dans Colab afin de bénéficier des capacités de calcul nécessaires pour mener à bien les tâches.**\n",
    "\n",
    "\n",
    "*Nous aborderons les points suivants : :*\n",
    "\n",
    "    - Comment utiliser un logiciel de chaine de prompt ?\n",
    "    - Comment lancer un agent et une conversation avec un LLM ?\n",
    "    \n",
    " ### Bibliographie :\n",
    " \n",
    " \n",
    " - [Reasoning & Thought](https://arxiv.org/abs/2212.09597) (review)\n",
    " - [LangChain](https://python.langchain.com/docs/get_started/introduction.html) et [ReAct](https://arxiv.org/abs/2210.03629)\n",
    " - [Tutoriel](https://larevueia.fr/langchain-le-guide-essentiel/) et [vidéo](https://youtu.be/mAoNANPOsd0)\n",
    " \n",
    " \n",
    " ### Prérequis : \n",
    "\n",
    "    - Python 3.8\n",
    "    - LangChain\n",
    "    - 8Gb RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeec738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c46918",
   "metadata": {},
   "source": [
    "**Pour Colab :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1667fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers einops accelerate langchain bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e04dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3489d92",
   "metadata": {},
   "source": [
    "## Import Model :\n",
    "\n",
    "Nous utilisons un modèle \"instruct\" ajusté pour la compréhension des instructions et est spécialisé dans l'interaction humaine, notamment dans les discussions (Chat). Le modèle *Falcon-7B-instruct* est efficace pour les techniques de prompts d'instruction, pour plus de détail, voir la partie conclusion et perspective. Les calculs ont été effectués sur Google Colab en utilisant une carte graphique NVIDIA Tesla T4.\n",
    "\n",
    "LangChain est un framework qui simplifie la création d'applications exploitant de vastes modèles de langage. Une des fonctionnalités clés de LangChain est sa capacité à interagir avec les pipelines de modèles HuggingFace, facilitant ainsi l'utilisation de ces modèles dans le processus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4303dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9860d2",
   "metadata": {},
   "source": [
    "Pour implémenter le modele Falcon-7B-Instruct, nous faisons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab91c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"tiiuae/falcon-7b-instruct\" #tiiuae/falcon-40b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0cf20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60ecbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline(\n",
    "    \"text-generation\", #task\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02372e7e",
   "metadata": {},
   "source": [
    "Pour préparer un modèle de pipeline afin de l'utiliser avec LangChain, nous faisons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd4105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e28920",
   "metadata": {},
   "source": [
    "Le paramètre de température ajuste le caractère aléatoire de la sortie, à 0, le modèle n'a pas d'aléatoire.\n",
    "\n",
    "## Automatize Prompt :\n",
    "\n",
    "LangChain offre une fonctionnalité essentielle : l'automatisation de la construction de chaînes de prompts. Les étapes de la chaîne correspondent au raisonnement du modèle de langage. Avec LangChain, il est possible de définir un prompt \"fixe\" contenant des variables d'entrée, que l'on peut indiquer sous la forme d'une chaîne de caractères formatée. Cela permet une flexibilité dans l'interaction avec le modèle et facilite l'adaptation du prompt en fonction des besoins spécifiques de chaque demande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a051fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa365a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are an intelligent chatbot. Help the following question with brilliant answers.\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5581df15",
   "metadata": {},
   "source": [
    "Pour importer un modèle de langage pré-entraîné avec un prompt spécifique, vous pouvez suivre l'étape suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e450b896",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3183b2d0",
   "metadata": {},
   "source": [
    "Et enfin, pour générer le résultat :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ccdf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Explain what is Artificial Intellience as Nursery Rhymes \"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7d1fdf",
   "metadata": {},
   "source": [
    "C'est grâce à la composabilité de l'outil et des prompts que vous pouvez construire votre propre cas d'utilisation. Cette fonctionnalité vous permet de combiner différentes étapes et prompts pour créer des workflows personnalisés répondant à vos besoins spécifiques.\n",
    "\n",
    "### Agent : Math Solver example\n",
    "\n",
    "Un agent est un composant qui a accès à une suite d'outils et peut décider quel outil utiliser en fonction de l'entrée de l'utilisateur. Il existe deux principaux types d'agents : les \"agents d'action\" et les \"agents de planification et d'exécution\". Les agents d'action décident d'une action à entreprendre et exécutent cette action une étape à la fois.\n",
    "\n",
    "Cet outil nous permet, par exemple, de construire un agent capable d'interagir avec Internet pour effectuer des recherches et créer automatiquement une documentation (comme dans l'exemple de l'Agent-GPT), ainsi que d'implémenter une fonctionnalité pour exécuter du code (comme dans l'exemple de l'Auto-GPT). Dans notre cas, nous allons simplement créer un agent qui résout des problèmes de mathématiques. Pour importer et paramétrer des agents pour les mathématiques, vous pouvez suivre les étapes suivantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4487019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools(\n",
    "    ['llm-math'],\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119edd07",
   "metadata": {},
   "source": [
    "On initialiser l'agent de cette façon :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6d0885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1770d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_agent = initialize_agent(\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72afd082",
   "metadata": {},
   "source": [
    "L'agent utilisé, appelé \"zero-shot-react-description\", est basé sur le framework ReAct (Reasoning + Acting), présenté dans un article de recherche de 2022. Pour tester cet agent, vous pouvez suivre les étapes suivantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5daa4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_agent(\"Combient fait (4.5*2.1)^2.2 ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b89db6",
   "metadata": {},
   "source": [
    "Veuillez noter que cette approche n'est pas adaptée aux problèmes non mathématiques, car elle ne sera pas en mesure de trouver des valeurs numériques ou des mesures de similarité d'embedding*. Cependant, il existe d'autres agents adaptés à cette tâche. Pour ajouter un agent spécifique, vous pouvez suivre les étapes suivantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0f4c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_tool = Tool(\n",
    "    name='Language Model',\n",
    "    func=llm_chain.run,\n",
    "    description='use this tool for general purpose queries and logic'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fcc577",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(llm_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d78060",
   "metadata": {},
   "source": [
    "Avec cet agent, il est possible d'engager des conversations, mais il n'est pas spécifiquement conçu pour automatiser la mémorisation. Pour cette fonctionnalité, il existe un module plus avancé qui répond mieux à ce besoin.\n",
    "\n",
    "*Les embeddings sont des représentations numériques des relations entre les chaînes de texte, exprimées sous forme de vecteurs de nombres réels. La distance entre deux vecteurs est utilisée pour mesurer leur proximité ; plus la distance est petite, plus la similarité est élevée. \n",
    "\n",
    "### Memory : Conversation\n",
    "\n",
    "Pour réaliser des conversations, nous utilisons directement le module complet dédié suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b615110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15131b8",
   "metadata": {},
   "source": [
    "L'initialisation se fait de la manière suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c4902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(llm=llm)\n",
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac011d9",
   "metadata": {},
   "source": [
    "On observe deux paramètres importants : {history} et {input}. Le paramètre {history} contient les informations de la conversation précédente. Il existe trois types de mémoires disponibles :\n",
    "\n",
    "    - ConversationBufferMemory : cette option conserve l'intégralité de la conversation précédente, mais peut épuiser rapidement le nombre de jetons disponibles.\n",
    "    - ConversationSummaryMemory : cette option résume la conversation en quelques lignes, mais la qualité de synthèse dépend des capacités du modèle de langage.\n",
    "    - ConversationBufferWindowMemory : cette option sélectionne plusieurs séquences aléatoires de la conversation précédente pour en conserver une fenêtre de mémoire.\n",
    "\n",
    "Ces différents types de mémoires permettent de choisir la meilleure approche en fonction des besoins spécifiques de la conversation.\n",
    "\n",
    "## Conclusion et Perspective\n",
    "\n",
    "Nous avons abordé la génération de prompts et la construction d'agent LLM. En combinant ces concepts avec le fine-tuning, il est possible de construire un modèle d'instruction/chat créant des données, notamment avec l'exemple de l'auto-apprentissage (Voir [self-instruct](https://arxiv.org/abs/2109.02846)). En outre, LangChain pourrait simplifier également l'utilisation d'algorithmes tels que les \"[Tree of Thought](https://github.com/princeton-nlp/tree-of-thought-llm)\" (arbre de pensées) ou les \"[Forests of Thought](https://github.com/mrspiggot/forestOfThoughts)\" (forêts de pensées), qui offrent des approches structurées pour la génération de texte et la résolution de problèmes.\n",
    "\n",
    "Les modèles de langage LLM présentent encore certaines limitations dans leur capacité à généraliser les problèmes (Voir cet [article](https://arxiv.org/pdf/2302.14045.pdf)). C'est pourquoi l'approche des MLLM (MultiModal Large Language Model, voir [1](https://github.com/HenryHZY/Awesome-Multimodal-LLM) et [2](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)) devient pertinente en combinant des adaptateurs et un LLM en tant que base (backbone). Cette approche offre la possibilité d'ajouter des fonctionnalités supplémentaires telles que l'apprentissage par renforcement, la curiosité ou la modélisation de la causalité, ce qui pourrait considérablement améliorer la généralisation des modèles (en particulier dans le cadre des *General Game Playing*). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
