{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f94d8c",
   "metadata": {},
   "source": [
    "# Tuning LLM with Learning by Examples\n",
    "\n",
    "Adapter le modele génératif de langage pour ajouter un module \"auto-critic\". Concretement, cela consiste à ajouter une sortie à derniere sequence de decodeur. Entrainer le modele en figeant les poids du LLM (TransfertLearning), puis fineTuner le modele complet.\n",
    "\n",
    "![LLM-Critic](LLM-AutoCritic.png)\n",
    "\n",
    "Nous nous appuyons sur un grand modèle de langage (LLM) open-source, qui peut être affiné et adapté selon les besoins. Le processus d'ajustement se déroule en deux-trois étapes : tout d'abord, nous ajustons les poids du réseau en fonction d'un problème spécifique, puis nous optimisons le \"prompt\" à fournir au réseau en amont afin de répondre au mieux à la demande. Enfin, il est possible de \"performer\" le modèle par des outils d'auto-apprentissage.\n",
    "\n",
    "\n",
    "Dans cette première étape, nous aborderons un cas d'école pour illustrer le processus. Nous verrons ensuite comment construire le jeu de données pour atteindre notre objectif. Ce tutoriel est basé sur le modèle [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) et utilise le guide Colab suivant : https://colab.research.google.com/drive/1Vvju5kOyBsDr7RX_YAvp6ZsSOoSMjhKD. Pour l'entraînement et les inférences, nous utiliserons Colab, car il nous donne gratuitement accès à un GPU T4 (la durée d'accès gratuite est à définir) et à un prix avantageux pour des GPU plus puissants tels que l'A100 et le V100.\n",
    "\n",
    "*Nous aborderons les points suivants : :*\n",
    "\n",
    "    - Comment utiliser un modèle de langage pré-entraîné pour la génération de texte ?\n",
    "    - Comment créer des données d'entraînement pour le fine-tuning ?\n",
    "    - Comment lancer l'ajustement d'un modèle LLM pour une problématique d'instruction classique ?\n",
    "    \n",
    " ### Bibliographie :\n",
    " \n",
    " \n",
    " - [LLaMA](https://arxiv.org/abs/2302.13971v1)\n",
    " - [Datasets](https://arxiv.org/abs/2109.02846)\n",
    " - [QLoRA](https://arxiv.org/abs/2305.14314)\n",
    " \n",
    " \n",
    " ### Prérequis : \n",
    "\n",
    "\n",
    "    - Python 3.8\n",
    "    - Pytorch 2.0\n",
    "    - 8Gb RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e28851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3efaa3",
   "metadata": {},
   "source": [
    "## Import Model\n",
    "\n",
    "La première étape consiste à importer le modèle pré-entraîné, offrant deux options avec **PyTorch** : utiliser directement la bibliothèque en chargeant les paramètres pré-entraînés depuis les fichiers \"bin\" téléchargés, ou instancier le modèle LLM souhaité et charger les \"checkpoints\" au format \"dict\". Dans le premier cas, on crée une instance du modèle et on charge les poids à partir des fichiers \"bin\". Dans le second cas, on instancie le modèle LLM et on le charge avec les \"checkpoints\". Assurez-vous de spécifier correctement les chemins des fichiers requis. Remplacez \"pytorch_model\" par la classe correspondante au modèle LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f1f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Par définition du modèle\n",
    "model = MyModel()\n",
    "# Chargement des checkpoints\n",
    "checkpoint = torch.load(\"chemin/vers/le/fichier.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "### Par le binaire\n",
    "model = torch.load('pytorch_model.bin', map_location='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc31159",
   "metadata": {},
   "source": [
    "Une autre possibilité est d'utiliser la bibliothèque clé en main \"transformers\" de **Hugging Face**. L'avantage de cette approche est qu'elle simplifie le processus en une seule étape et offre des fonctions prédéfinies pour le fine-tuning, ce qui facilite grandement son utilisation. Par conséquent, nous utiliserons exclusivement cet outil pour nos besoins de fine-tuning à l'avenir, car il offre une solution pratique et efficace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3bb6436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab92747",
   "metadata": {},
   "source": [
    "Pour importer un modèle pré-entraîné et visualiser sa structure de réseau, vous pouvez suivre cette étape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84d09660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c7fb0fd15a4ed080749551621aa3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=31999)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\", trust_remote_code=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3605e28",
   "metadata": {},
   "source": [
    "Le modèle LLaMA, bien qu'un des premiers modèles LLM \"décodeur seulement\" pour la complétion de texte disponibles en local, présente des limitations de licence. Par conséquent, nous opterons plutôt pour le modèle Falcon-7B, basé sur le modèle BLOOM, qui offre une alternative plus favorable. À partir de ce point, nous utiliserons exclusivement le modèle Falcon-7B pour nos besoins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e44dd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa2dc95bd9f448cbdcfb8d5dd41cf2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RWForCausalLM(\n",
      "  (transformer): RWModel(\n",
      "    (word_embeddings): Embedding(65024, 4544)\n",
      "    (h): ModuleList(\n",
      "      (0-31): 32 x DecoderLayer(\n",
      "        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): Attention(\n",
      "          (maybe_rotary): RotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=4544, out_features=4672, bias=False)\n",
      "          (dense): Linear(in_features=4544, out_features=4544, bias=False)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (dense_h_to_4h): Linear(in_features=4544, out_features=18176, bias=False)\n",
      "          (act): GELU(approximate='none')\n",
      "          (dense_4h_to_h): Linear(in_features=18176, out_features=4544, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b1736",
   "metadata": {},
   "source": [
    "Ensuite, pour définir la structure des données d'entrée pour notre modèle, nous devons segmenter les mots en unités plus petites appelées \"tokens\". Pour ce faire, nous utilisons une méthode appelée \"AutoTokenizer\" qui convertit une phrase en un ensemble de \"tokens\" en utilisant un modèle spécifique. Chaque modèle dispose d'un \"Tokenizer\" spécifique, et son utilisation se fait de la manière suivante : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9ae820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\n",
    "input_ = tokenizer(\"Girafatron is nice.\\nFabien: Hello, Girafatron!\\nGirafatron:\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e34b7",
   "metadata": {},
   "source": [
    "Une fois que la séquence a été convertie pour être utilisée dans un modèle, nous pouvons utiliser la fonction \"generate\" pour compléter la séquence en générant du texte supplémentaire. Cette fonction permet au modèle de prédire les mots ou les phrases suivantes en fonction du contexte fourni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a90d8a37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Input length of input_ids is 26, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    }
   ],
   "source": [
    "output_ids = model.generate(input_.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb29a6",
   "metadata": {},
   "source": [
    "Enfin, nous décodons et affichons le résultat de la génération en utilisant à nouveau le tokenizer. Cela nous permet de convertir les \"tokens\" générés par le modèle en texte lisible. En utilisant le tokenizer, nous pouvons restaurer la séquence de mots ou de phrases complétées pour l'afficher et l'examiner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35b08936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Girafatron is nice.\n",
      "Fabien: Hello, Girafatron!\n",
      "Girafatron: Hello\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4063f6e2",
   "metadata": {},
   "source": [
    "Il est important de noter que la génération de texte peut être un processus très intensif, en particulier sur un ordinateur avec une quantité limitée de RAM. Pour remédier à cela, une approche consiste à \"quantifier\" le modèle en convertissant ses poids dans un format de données plus compact. Cette technique permet de réduire la consommation de mémoire et d'accélérer le processus de génération. Nous utiliserons cette méthode lors du fine-tuning du modèle pour optimiser ses performances.\n",
    "\n",
    "## Prepare Fine Tuning\n",
    "\n",
    "Le fine-tuning est une technique d'apprentissage par transfert qui consiste à prendre un modèle pré-entraîné et à y apporter des modifications. Il existe différentes stratégies de fine-tuning : certaines consistent à ajouter un nouveau modèle à entraîner entièrement à la suite du modèle pré-entraîné, tandis que d'autres impliquent une modification directe des poids du réseau pré-entraîné. Le choix de la stratégie dépend des besoins spécifiques de la tâche et des ressources disponibles.\n",
    "\n",
    "Pour faciliter le processus de fine-tuning, il existe une bibliothèque tout-en-un de **Hugging Face** appelée \"peft\" (Parameter-Efficient Fine-Tuning). Cette bibliothèque offre des fonctionnalités spécifiquement conçues pour le fine-tuning des modèles génératif, permettant ainsi une implémentation simplifiée et efficace de cette étape cruciale. En utilisant \"peft\", vous pouvez bénéficier de diverses fonctionnalités telles que la gestion des données d'entraînement, l'optimisation des hyperparamètres et la création de boucles d'entraînement, ce qui facilite grandement le processus de fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb0ead74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin C:\\Python38\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "CUDA SETUP: Loading binary C:\\Python38\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\n",
      "argument of type 'WindowsPath' is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python38\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "import peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b3d056",
   "metadata": {},
   "source": [
    "Pour convertir le modèle pour réduire la taille par quantification, nous faisons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55a7358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model_kbit = peft.prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d75c693",
   "metadata": {},
   "source": [
    "Dans le cadre du fine-tuning, nous allons modifier directement les poids du réseau, en nous concentrant uniquement sur ceux qui n'ont pas d'effet significatif sur la sortie ou qui ont le même effet. Pour cela, nous construisons une fonction spécifique qui nous permet de déterminer quels poids peuvent être modifiés dans notre modèle. Cette approche ciblée nous permet de concentrer l'entrainement sur les parties du modèle qui ont le plus d'impact sur la tâche spécifique que nous souhaitons accomplir, tout en préservant les parties du modèle qui sont déjà efficaces et bien entraînées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbd1365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695e2a89",
   "metadata": {},
   "source": [
    "Pour le fine-tuning, nous utilisons l'algorithme LoRa (Low Rank), qui nous permet de déterminer les poids modifiables dans notre modèle. L'algorithme LoRa identifie les poids qui ont un impact significatif sur la sortie du modèle et sélectionne les poids qui ont une influence moindre ou similaire. En réduisant le rang des poids, nous pouvons réduire la dimensionnalité du modèle d'entrainement, ce qui facilite le fine-tuning en réduisant la complexité et en améliorant l'efficacité du processus d'entraînement. Cette approche permet d'optimiser les performances du modèle tout en limitant le nombre de poids à ajuster, ce qui peut être particulièrement utile lorsque les ressources computationnelles sont limitées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c201a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2359296 || all params: 6924080000 || trainable%: 0.03407378308742822\n"
     ]
    }
   ],
   "source": [
    "config = peft.LoraConfig(r=8, lora_alpha=32, target_modules=[\"query_key_value\"],  lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "\n",
    "model_kbit = peft.get_peft_model(model_kbit, config)\n",
    "print_trainable_parameters(model_kbit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a370fe5",
   "metadata": {},
   "source": [
    "L'avantage est que seuls 0,03% des poids du modèle doivent être modifiés. Cette proportion faible réduit considérablement les exigences en termes de ressources de calcul. Nous pouvons donc effectuer le fine-tuning de manière efficace, même avec des ressources limitées, tout en préservant les performances du modèle pré-entraîné. Cela permet des ajustements précis et rapides pour répondre aux besoins spécifiques de la tâche sans nécessiter une grande puissance de calcul.\n",
    "\n",
    "### Prepare Data\n",
    "\n",
    "Pour entraîner notre modèle, nous avons besoin de données d'entraînement qui correspondent à la tâche spécifique que nous souhaitons accomplir. Pour cela, nous utilisons le module \"datasets\" qui offre un ensemble de jeux de données pré-construits. Cependant, il est également possible de construire des données spécifiques pour une application particulière si nécessaire. En utilisant le module \"datasets\", nous pouvons accéder à des ensembles de données diversifiés et bien organisés, ce qui facilite le processus d'entraînement et d'évaluation de notre modèle. Il est important de sélectionner ou de créer des données d'entraînement de haute qualité et représentatives de la tâche que nous souhaitons résoudre afin d'obtenir les meilleurs résultats possibles lors du fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8a9c7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210536f7",
   "metadata": {},
   "source": [
    "Dans notre cas d'école, nous allons utiliser des données de citations comprenant un titre, une citation correspondante et l'auteur. Notre objectif sera de prédire une nouvelle citation possible à partir d'un titre donné. Pour préparer ces données, nous devons les tokenizer, c'est-à-dire les convertir en une représentation numérique compréhensible par le modèle. Nous utilisons donc une fonction de tokenization spécifique pour diviser les titres et les citations en unités appelées \"tokens\". Cette étape de tokenization nous permet pour préparer les données d'entrée au modèle et de les traiter de manière adéquate lors du processus d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "140c87dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/ffurfaro/.cache/huggingface/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4c53462cb14f63a2ef08c911845a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\ffurfaro\\.cache\\huggingface\\datasets\\Abirate___json\\Abirate--english_quotes-6e72855d06356857\\0.0.0\\e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4\\cache-b19fbca03c240ead.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 2508}\n"
     ]
    }
   ],
   "source": [
    "data = datasets.load_dataset(\"Abirate/english_quotes\")\n",
    "data_tokenized = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n",
    "print(data.num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf8d991",
   "metadata": {},
   "source": [
    "Pour visualiser les \"premières données\", nous faisons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d68c2730",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    \"“Be yourself; everyone else is already taken.”\",\n",
      "    \"“I'm selfish, impatient and a little insecure. I make mistakes, I am out of control and at times hard to handle. But if you can't handle me at my worst, then you sure as hell don't deserve me at my best.”\",\n",
      "    \"“Two things are infinite: the universe and human stupidity; and I'm not sure about the universe.”\"\n",
      "  ]\n",
      "] [\n",
      "  [\n",
      "    \"Oscar Wilde\",\n",
      "    \"Marilyn Monroe\",\n",
      "    \"Albert Einstein\"\n",
      "  ]\n",
      "] [\n",
      "  [\n",
      "    [\n",
      "      \"be-yourself\",\n",
      "      \"gilbert-perreira\",\n",
      "      \"honesty\",\n",
      "      \"inspirational\",\n",
      "      \"misattributed-oscar-wilde\",\n",
      "      \"quote-investigator\"\n",
      "    ],\n",
      "    [\n",
      "      \"best\",\n",
      "      \"life\",\n",
      "      \"love\",\n",
      "      \"mistakes\",\n",
      "      \"out-of-control\",\n",
      "      \"truth\",\n",
      "      \"worst\"\n",
      "    ],\n",
      "    [\n",
      "      \"human-nature\",\n",
      "      \"humor\",\n",
      "      \"infinity\",\n",
      "      \"philosophy\",\n",
      "      \"science\",\n",
      "      \"stupidity\",\n",
      "      \"universe\"\n",
      "    ]\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(data_tokenized.data['train'][0][:3], data_tokenized.data['train'][1][:3], data_tokenized.data['train'][2][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9835a842",
   "metadata": {},
   "source": [
    "Bien que notre ensemble de données soit limité à seulement 2500 exemples, il est suffisant pour notre cas d'utilisation spécifique. Malgré sa taille réduite, cet ensemble de données couvre une gamme variée de titres et de citations, ce qui nous permettra de former un modèle capable de générer des citations pertinentes en fonction des titres fournis.\n",
    "\n",
    "### Train\n",
    "\n",
    "Lors de l'entraînement, nous utilisons les paramètres suivants pour optimiser notre modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e36693c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for falcon\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model_kbit,\n",
    "    train_dataset=data_tokenized[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=10,\n",
    "        learning_rate=2e-4,\n",
    "        #fp16=True, # if cuda device\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db51e65",
   "metadata": {},
   "source": [
    "Pour lancer l'entraînement de notre modèle en utilisant la bibliothèque Hugging Face, nous utilisons la fonction suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490dd606",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kbit.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f3f2d2",
   "metadata": {},
   "source": [
    "Pour mieux comprendre cette étape, regardez la vidéo de [1littlecoder](https://youtu.be/NRVaRXDoI3g).\n",
    "\n",
    "### Save Model et Import\n",
    "\n",
    "Pour enregistrer les poids modifiés de notre modèle, nous utilisons la méthode fournie par la bibliothèque Hugging Face. Cela nous permet de sauvegarder les paramètres ajustés du réseau dans un format approprié, tel qu'un fichier binaire ou un dictionnaire, en fonction de nos besoins. L'enregistrement des poids modifiés nous permet de conserver les modifications apportées lors du processus de fine-tuning, afin de pouvoir les réutiliser ultérieurement pour des inférences ou pour continuer l'entraînement du modèle. Cette étape est essentielle pour préserver et partager les résultats de notre travail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5b22baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e7ddc7",
   "metadata": {},
   "source": [
    "Une fois que nous avons enregistré les poids modifiés de notre modèle, nous pouvons facilement les importer en utilisant une fonction dédiée. Cette fonction nous permet de charger spécifiquement les poids que nous avons ajustés, en les extrayant du fichier ou du dictionnaire dans lequel ils ont été enregistrés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ff86bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig.from_pretrained('outputs')\n",
    "model_trained = get_peft_model(model_kbit, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b921be37",
   "metadata": {},
   "source": [
    "En important uniquement les poids modifiés, nous pouvons réutiliser notre modèle fine-tuné pour effectuer des inférences ou poursuivre l'entraînement sans avoir besoin de reconfigurer l'ensemble du réseau. Cela nous offre une flexibilité et une efficacité accrues dans l'utilisation de notre modèle pré-entrainé et adapté à notre tâche spécifique.\n",
    "\n",
    "## Conclusion et Perspective\n",
    "\n",
    "Il est intéressant de constater que même avec un ensemble de données d'entraînement relativement restreint, il est possible d'optimiser efficacement un modèle de langage LLM pré-entraîné pour une utilisation spécifique. Cette approche de fine-tuning permet d'adapter le modèle existant à notre tâche spécifique de manière pratique et efficiente. Grâce à cette méthode, il est possible de bénéficier des connaissances préalables du modèle tout en l'adaptant pour obtenir des résultats pertinents et précis dans notre domaine d'intérêt. Cela démontre la flexibilité des techniques de fine-tuning dans le domaine de l'apprentissage automatique des LLM et ouvre des possibilités intéressantes pour l'optimisation de modèles pré-entraînés dans diverses applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
